<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenShift on Purplecarrot</title>
    <link>https://purplecarrot.co.uk/tags/openshift/</link>
    <description>Recent content in OpenShift on Purplecarrot</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Sat, 30 Oct 2021 11:04:12 +0000</lastBuildDate><atom:link href="https://purplecarrot.co.uk/tags/openshift/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Breaking into your OpenShift Cluster</title>
      <link>https://purplecarrot.co.uk/post/2021-10-30-breaking_into_your_openshift_cluster/</link>
      <pubDate>Sat, 30 Oct 2021 11:04:12 +0000</pubDate>
      
      <guid>https://purplecarrot.co.uk/post/2021-10-30-breaking_into_your_openshift_cluster/</guid>
      <description>
        
          &lt;p&gt;Of course, this really shouldn&#39;t happen right? You&#39;re a responsible IT professional, and your OpenShift cluster is configured with multiple authentication methods, and a secure backup of a &lt;code&gt;kubeconfig&lt;/code&gt; file with the system:admin user certificates for passwordless login? Except sometimes it does.&lt;/p&gt;
&lt;p&gt;In my day job, we have a small lab OCP cluster running on vSphere that might not get used for weeks or sometimes months. The OpenShift nodes that make up the OCP cluster sometimes get automatically powered down and left off in that powered off state.&lt;/p&gt;
&lt;p&gt;Yesterday, I came to use this cluster for the first time again after quite a long period and found that I was unable to login with a standard LDAP user account with cluster-admin credentials that I would normally use. In addition, my admin &lt;code&gt;kubeconfig&lt;/code&gt; (the one generated by openshift-installer for the original installation), had very old pre-rotation CA certs or something and hadn&#39;t been used for a while. Attempting to use it just gave x509 errors and so it seemed unusable. This has happened a couple of times now over the years, and I believe it&#39;s related to it being an IPI cluster and certificate rotation issues from the cluster being powered down for long periods. Before looking at the root cause, I&#39;m just writing down the recovery procedure I used for next time!&lt;/p&gt;
&lt;p&gt;So with no standard OAUTH authentication method available, I went back trying to use the original kubeconfig from the installation of the cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ export KUBECONFIG=auth/kubeconfig
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ oc get nodes
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;Unable to connect to the server: x509: certificate signed by unknown authority
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To try and understand more about what was happening, I added &lt;code&gt;--insecure-skip-tls-verify=true&lt;/code&gt;, and the &lt;code&gt;--loglevel=10&lt;/code&gt; options to the command line, but I could see that a goroutine stack trace was immediately output after the x509 error message above.&lt;/p&gt;
&lt;p&gt;As any experienced senior engineer will know, the first place to go for solving any technical problem is a good Google or StackOverflow search. Unfortunately StackOverflow threw up a lot of articles that were old, or weren&#39;t quite right and not what I was looking for.&lt;/p&gt;
&lt;p&gt;Unsurprisingly - this being OpenShift - the best results were found in the &lt;a href=&#34;https://access.redhat.com/search/#/&#34;&gt;RedHat Knowledgebase&lt;/a&gt;. The KB articles &lt;a href=&#34;https://access.redhat.com/solutions/4505101&#34;&gt;error: x509 certificate signed by unknown authority when logging in OpenShift 4 using the installation kubeconfig file&lt;/a&gt; and &lt;a href=&#34;https://access.redhat.com/solutions/5286371&#34;&gt;How to re create kubeconfig from scratch for system:admin user in OpenShift 4&lt;/a&gt; looked promising, but both required existing access to the cluster to recreate them, which I didn&#39;t have because standard authentication methods were not working.&lt;/p&gt;
&lt;p&gt;I knew that all the certs I needed were on the cluster nodes themselves, but these systems were running RedHat CoreOS. RedHat CoreOS is based on RedHat Enterprise Linux, but designed very much as an appliance. It&#39;s not expected to be managed by a human sysadmin - it has disabled root account, doesn&#39;t allow passwordless logins and its OS configuration is managed by the MachineConfigOperator function of OpenShift. The next problem was that (for reasons of this being a lab and the cluster being built by a colleague) the SSH key for the coreos user used for the installation wasn&#39;t available to me either :-) I did however, have access to the vSphere console of these VMS. There was no option, I had break my way in and get those certs.&lt;/p&gt;
&lt;p&gt;The first thing was to boot the system to single user, reset the root password so I could login on the console. I&#39;ve done this thousands of times over the years on many different versions of Linux and flavours of Linux, but interestingly this was more complicated on CoreOS (because of the way CoreOS works, and so it&#39;s not normally required). However, I did eventually get in and then I was able to locate the files I needed.&lt;/p&gt;
&lt;p&gt;So all the secrets needed to run kube-apiserver are found in &lt;code&gt;/etc/kubenetes/static-pod-resources/kube-apiserver-certs&lt;/code&gt;. In particular, the node-kubeconfigs/ directory has mulitple kubeconfig files, one of which was called &lt;code&gt;lb-int.kubeconfig&lt;/code&gt;. When I set my KUBECONFIG environment variable to point to this file, I was then able to issue oc commands to the cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;# cd /etc/kubenetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;# export KUBECONFIG=$(pwd)/lb-int.kubeconfig
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;# oc get nodes
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;&amp;lt;lots of nodes with STATUS NotReady&amp;gt;
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;# oc get pods -A | grep -e Running
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;&amp;lt;lots of pods in ContainerCreating or Pending
&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;# oc get csr -o name | xargs oc adm certificate approve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I found that half the nodes were in state &lt;code&gt;NotReady&lt;/code&gt;, and so the oauth-openshift containers were not &lt;code&gt;Running&lt;/code&gt;. This in turn was because there were a bunch of unapproved CSRs. Once these were approved, the nodes became &lt;code&gt;Ready&lt;/code&gt; and the OAuth authentication pods were able to start up and the cluster recovered itself in the usual way. I was then able to login normally again and recreate a backup &lt;code&gt;kubeconfig&lt;/code&gt;!&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Does Alpine resolve DNS properly?</title>
      <link>https://purplecarrot.co.uk/post/2021-09-04-does_alpine-resolve_dns_properly/</link>
      <pubDate>Sat, 04 Sep 2021 08:51:20 +0000</pubDate>
      
      <guid>https://purplecarrot.co.uk/post/2021-09-04-does_alpine-resolve_dns_properly/</guid>
      <description>
        
          &lt;p&gt;To anybody who has ever used containers, Alpine is a well known base operating system layer to many popular and widely used container images in use today. By design, it&#39;s a cut down minimal OS layer using the excellent &lt;a href=&#34;https://www.busybox.net/&#34;&gt;busybox&lt;/a&gt; and &lt;a href=&#34;https://musl.libc.org/&#34;&gt;musl libc&lt;/a&gt; C library. By using these alternatives to GNU Coreutils and GNU glibc C Library, it provides a base OS container layer that allows you to build and run very small Linux containers.&lt;/p&gt;
&lt;p&gt;This week I was asked to look at a problem where an application team could not resolve a key DNS name used on our internal network when running their application within the container - their first thought was a problem with the OpenShift/Kubernetes environment where the container was running.&lt;/p&gt;
&lt;p&gt;Unfortunately, the company I work for doesn&#39;t like us to disclose internal proprietary network information, so for the purposes of the post, let&#39;s say the DNS name being queried is &lt;code&gt;vis.company.com&lt;/code&gt; (very important service). All IPs and some other data in the extracts below have been manually changed to private RFC1918 addresses or redacted too.&lt;/p&gt;
&lt;h2 id=&#34;basic-troubleshooting&#34;&gt;Basic Troubleshooting&lt;/h2&gt;
&lt;p&gt;The first thing to do was to perform my own DNS lookup of &lt;code&gt;vis.company.com&lt;/code&gt;. This worked fine from both my Linux workstation and Windows laptop.&lt;/p&gt;
&lt;p&gt;Now what made this problem interesting was that the application running in the container was able to resolve all other DNS entries that it was required to in order to function(eg &lt;code&gt;a.company.com&lt;/code&gt;, &lt;code&gt;b.company.com&lt;/code&gt;), but it couldn&#39;t resolve this one crucial DNS entry &lt;code&gt;vis.company.com&lt;/code&gt;. When you connected to a shell running inside the container and ran &lt;code&gt;getent hosts vis.company.com&lt;/code&gt; it simply returned nothing at all, and exited with exit code 2.&lt;/p&gt;
&lt;h2 id=&#34;inside-the-container&#34;&gt;Inside the Container&lt;/h2&gt;
&lt;p&gt;So how do you go about troubleshooting a problem like this from within a container? In a standard OS, you can simply use the standard tools in your sysadmin toolkit - ss (lsof/netstat), dig, strace, tcpdump, nmap, etc - but inside a container these are unlikely to be available by default (and some containers don&#39;t even include a minimal unix shell even). I have a pre-built container I use for situations like this that I then add as a sidecar in the pod I&#39;m troubleshooting. Then by setting &lt;code&gt;shareProcessNamespace: true&lt;/code&gt; in the podspec, it will allow you to easily debug and strace processes running inside the first container from the sidecar.&lt;/p&gt;
&lt;h2 id=&#34;dns-query&#34;&gt;DNS Query&lt;/h2&gt;
&lt;p&gt;So strace&#39;ing the DNS queries I could see the &lt;code&gt;socket()&lt;/code&gt; and &lt;code&gt;bind()&lt;/code&gt; calls, followed by receiving a response from the DNS server. However, this response was empty. Something weird was happening, because running the same command on a standard Linux host you could see 100s of lines more of system calls and a good DNS response with multiple A records was returned.&lt;/p&gt;
&lt;p&gt;So the next step was to use tcpdump and capture a packet trace of the DNS query. Below is the relevant section from that pcap that shows the DNS query:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;18:00:27.376761 IP (tos 0x0, ttl 64, id 32569, offset 0, flags [DF], proto UDP (17), length 68)
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;    10.96.10.99.43507 &amp;gt; 10.96.0.10:domain: 19977+ A? vis.company.com. (40)
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;18:00:27.386585 IP (tos 0x0, ttl 64, id 24288, offset 0, flags [DF], proto UDP (17), length 79)
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;    10.96.0.10.domain &amp;gt; 10.96.10.99.43507: 19977| 0/0/1 (51)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So what does this tell us?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This is standard UDP DNS query (id 19977) to query the IPv4 record &lt;code&gt;vis.company.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The DNS server response comes from &lt;code&gt;10.96.0.10&lt;/code&gt;, which is the Kubernetes &lt;code&gt;Service&lt;/code&gt; address for CoreDNS.&lt;/li&gt;
&lt;li&gt;A DNS response was received, but no DNS records were in it (&lt;code&gt;0/0/1&lt;/code&gt; = 0 answer records, 0 nameservers and 1 additional record which is the query address itself)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;|&lt;/code&gt; character after the 19977 id indicates that the TC (TrunCation) bit is set on this packet.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most interesting of these is the last. The TC bit is used when the DNS response the DNS server wants to send to the client is longer than the 512bytes available to it in a UDP packet (see &lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc1035&#34;&gt;RFC1035&lt;/a&gt; for more information.)&lt;/p&gt;
&lt;p&gt;This is a signal to the DNS resolver client that it needs to switch from a standard UDP DNS query and do a new TCP DNS query instead - but in both the strace and tcpdump output of the application running the the Alpine container, the resolver query exited immediate after receiving this TC UDP packet from the DNS server.&lt;/p&gt;
&lt;p&gt;I expected this to be a bug, but it turned out that this is a &lt;a href=&#34;https://wiki.musl-libc.org/functional-differences-from-glibc.html&#34;&gt;functional difference between musl libc and glibc&lt;/a&gt; and is by design. The musl libc author states that he intentionally didn&#39;t support TCP as felt it would be better for performance and UX reasons:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My choice not to do TCP in musl&amp;#39;s stub resolver was based on an interpretation that truncated results are not just acceptable but better ux - not only do you save major round-trip delays to DNS but you also get a reasonable upper bound on # of addrs in result.&lt;/p&gt;&amp;mdash; Rich Felker (@RichFelker) &lt;a href=&#34;https://twitter.com/RichFelker/status/994629795551031296?ref_src=twsrc%5Etfw&#34;&gt;May 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is true, and a valid point. However, it means that this particular app would never work when running in a container based on Alpine. Sometimes predictable functionality is more important.&lt;/p&gt;
&lt;p&gt;There are various other ways this could have been made to work. Perhaps making the DNS entry smaller - which was indeed unnecessarily large - would indeed be a better fix, but unfortunately this wasn&#39;t a practical option as the DNS entry wasn&#39;t under this team&#39;s control.&lt;/p&gt;
&lt;p&gt;Instead, the quickest fix was to rebuild the application into a new container image using the RedHat UBI8 image as a base layer instead of Alpine. This image uses glibc resolver, and then the app then ran fine, in the same way as it did on the RedHat Linux 8 host where it had ran before being containerized.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Getting Along with the OpenShift Machine Config Operator</title>
      <link>https://purplecarrot.co.uk/post/2021-12-19-machineconfigoperator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://purplecarrot.co.uk/post/2021-12-19-machineconfigoperator/</guid>
      <description>
        
          &lt;p&gt;When I&#39;m working with the Machine Config Operator (MCO) in OpenShift, and frequently typing &lt;code&gt;oc describe mcp&lt;/code&gt; and &lt;code&gt;oc get mcp&lt;/code&gt;, I&#39;m often reminded of the MCP (Machine Control Program) in the film &lt;a href=&#34;https://en.wikipedia.org/wiki/Tron&#34;&gt;Tron&lt;/a&gt;. I like the idea behind the MCO, and when it&#39;s working it&#39;s great, but perhaps this association in my mind is not only because of the shared acronym, but because you do sometimes feel you&#39;re fighting with it for control of your cluster!&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-openshift-machine-config-operator&#34;&gt;What is the OpenShift Machine Config Operator?&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/openshift/machine-config-operator&#34;&gt;OpenShift Machine Config Operator&lt;/a&gt; is a very powerful piece of software and an important part of the OpenShift Container Platform (OCP). It manages the Operating System versions, updates and configuration files of the master and worker nodes that make up an OpenShift cluster. Operating system configuration files such as NetworkManager config files, systemd units, sysctl files as well as kubelet config files and certificate bundles, are base64 encoded and held in &lt;code&gt;MachineConfig&lt;/code&gt; resources inside Kubernetes itself.&lt;/p&gt;
&lt;p&gt;This might not sound that interesting in the context of today&#39;s public cloud service providers IaaS solutions, but when the nodes are running on HP and Dell bare metal physical servers in private disconnected data centres, having a software operator like this - which is run and managed using native Kubernetes primitives and commands - drastically reduces the SA workload required to manage and maintain multiple clusters made up of hundreds of bare metal physical servers&lt;/p&gt;
&lt;h2 id=&#34;understanding-the-machine-config-operator-mco&#34;&gt;Understanding the Machine Config Operator (MCO)&lt;/h2&gt;
&lt;p&gt;The MCO lives in the &lt;em&gt;openshift-machine-config-operator&lt;/em&gt; project and is present in every OpenShift cluster. There are a number of different containers running in this namespace:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;machine-config-operator&lt;/strong&gt; - this is the main controller loop or ClusterOperator itself that deploys and manages everything else in this namespace.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;machine-config-server&lt;/strong&gt; - provides the endpoint that nodes connect to in order to get their configuration files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;machine-config-controller&lt;/strong&gt; - co-ordinates upgrades and manages the lifecycle of nodes in the cluster by rendering (generating) &lt;code&gt;MachineConfigs (mc)&lt;/code&gt;, managing &lt;code&gt;MachineConfigPools (mcp)&lt;/code&gt; and by co-ordinating with the &lt;em&gt;machine-config-daemon&lt;/em&gt; running on each node to keep all the nodes up to date with the correct configuration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;machine-config-daemon&lt;/strong&gt; - responsible for updating nodes to a given &lt;code&gt;MachineConfig&lt;/code&gt; when requested to by the machine-config-controller. This runs as a DaemonSet so there is one pod per node in the cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The terminal output below illustrates these pods running in the &lt;em&gt;openshift-machine-config-operator&lt;/em&gt; project in an OpenShift cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;$ oc get pods -n openshift-machine-config-operator -o wide
&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;NAME                                         READY   STATUS    RESTARTS   AGE   IP              NODE       NOMINATED NODE   READINESS GATES
&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;machine-config-controller-7c4975f766-gbnh2   1/1     Running   1          20h   10.129.0.27     server01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;machine-config-daemon-2kt5g                  2/2     Running   2          18h   192.162.1.203   server03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;machine-config-daemon-jmhll                  2/2     Running   6          54d   192.168.1.206   server06   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;machine-config-daemon-rzwhv                  2/2     Running   6          54d   192.168.1.202   server02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;machine-config-daemon-vqphs                  2/2     Running   24         54d   192.168.1.201   server01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;machine-config-daemon-xqjgk                  2/2     Running   2          17h   192.168.1.205   server05   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;machine-config-operator-66f4cd998f-v8cfg     1/1     Running   1          20h   10.129.0.40     server01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;machine-config-server-5hlsd                  1/1     Running   12         54d   192.168.1.101   server01   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;machine-config-server-dfvg8                  1/1     Running   3          54d   192.168.1.102   server02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;machine-config-server-tr2fd                  1/1     Running   11         54d   192.168.1.103   server03   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;machineconfigs&#34;&gt;MachineConfigs&lt;/h2&gt;
&lt;p&gt;So how does an update or change to a systemd unit file used to start the kubelet on a node make it to the OS disk of a node that requires it?&lt;/p&gt;
&lt;p&gt;One of the core utilities underpinning the MCO is &lt;a href=&#34;https:/github.com/coreos/ignition&#34;&gt;Ignition&lt;/a&gt;. When RedHat acquired the CoreOS company in 2018, they merged the best parts of CoreOS, Red Hat Linux and Red Hat Atomic operating systems and the resulting product was named Red Hat CoreOS (RHCOS). This is the OS that runs on the master and worker nodes in an OpenShift cluster.&lt;/p&gt;
&lt;p&gt;All the OS configuration files a node requires are encoded and encapsulated in ignition files. The &lt;em&gt;machine-config-controller&lt;/em&gt; collates all these various snippets and OS configuration files each node requires, and then renders (generates) &lt;code&gt;MachineConfig&lt;/code&gt; kubernetes resources with all the files.&lt;/p&gt;
&lt;p&gt;The command below, shows the &lt;code&gt;MachineConfigs&lt;/code&gt; available in an OpenShift cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;$ oc get mc --sort-by=&amp;#39;{.metadata.creationTimestamp}&amp;#39;
&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;99-worker-ssh                                                                                 3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;99-master-ssh                                                                                 3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;00-master                                          a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;01-master-kubelet                                  a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;01-worker-container-runtime                        a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;01-master-container-runtime                        a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;99-master-generated-registries                     a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;00-worker                                          a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;99-worker-generated-registries                     a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;01-worker-kubelet                                  a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;rendered-master-0a540fd2eb85335cab8066f0bb407d5b   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;rendered-worker-9da78a560e7fa3c0cbd53d2bf0c17941   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             216d
&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;rendered-master-248f9034db99de75517a88a2ae8d29e2   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             215d
&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;rendered-worker-e323d4623ccb8cb71a4953f4ca81f8a4   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             215d
&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;rendered-worker-5599b4631efd88b5834f0844bec5ce83   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             203d
&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;rendered-master-8e5868011cc53d9697ac9dda69a64b39   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             203d
&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;rendered-worker-77e79ef013b565633a4d04da28f7dda4   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             203d
&lt;span class=&#34;ln&#34;&gt;20&lt;/span&gt;rendered-master-c40185b01423956d1146b7dea0a3e265   116603ff3d7a39c0de52d7d16fe307c8471330a0   3.2.0             203d
&lt;span class=&#34;ln&#34;&gt;21&lt;/span&gt;rendered-worker-dee67e9dc2ba77876de8bc8d045740dd   c76785d62cc28ddf3390c865c9e999a02248cd84   3.2.0             57d
&lt;span class=&#34;ln&#34;&gt;22&lt;/span&gt;rendered-master-3fcbbc2dcbddc79435994c98b655b192   c76785d62cc28ddf3390c865c9e999a02248cd84   3.2.0             57d
&lt;span class=&#34;ln&#34;&gt;23&lt;/span&gt;rendered-worker-7373393797c4c7e5ae500ff51ba234a9   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             55d
&lt;span class=&#34;ln&#34;&gt;24&lt;/span&gt;rendered-master-40ad8d11cc05c7079ee8f3bfa6df5540   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             55d
&lt;span class=&#34;ln&#34;&gt;25&lt;/span&gt;rendered-worker-53656729c24a74935907c4fba2adc90a   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             54d
&lt;span class=&#34;ln&#34;&gt;26&lt;/span&gt;rendered-master-1e2f18449f36cad8c9618c264d5cdc10   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             54d
&lt;span class=&#34;ln&#34;&gt;27&lt;/span&gt;rendered-worker-82df13b281ba3ddb9ba2e8811ef7957c   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             14d
&lt;span class=&#34;ln&#34;&gt;28&lt;/span&gt;rendered-master-7ed9a47c85ff478b341adaa20a55e942   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             14d
&lt;span class=&#34;ln&#34;&gt;29&lt;/span&gt;rendered-master-98968a4a639e3efad0436d19d4a6c93f   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             14d
&lt;span class=&#34;ln&#34;&gt;30&lt;/span&gt;rendered-worker-65ec18c5a3e4ebbf5f8ef459fb3c50b2   a537783ea4a0cd3b4fe2a02626ab27887307ea51   3.2.0             14d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this command output, you can see that 216 days ago when the cluster was first installed, initial &lt;code&gt;MachineConfigs&lt;/code&gt; were rendered - one for the master nodes (rendered-master-0a540f..) in the cluster, and one for the worker nodes (rendered-worker-9da78a...). Whilst these two &lt;code&gt;MachineConfigs&lt;/code&gt; share common OS configuration files, there are also lots of differences and so they are managed separately for each &lt;code&gt;MachineConfigPool&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Since the initial install, we can see at various intervals, new &lt;code&gt;MachineConfig&lt;/code&gt; objects were rendered by the &lt;em&gt;machine-config-controller&lt;/em&gt;. Each new rendered- &lt;code&gt;MachineConfig&lt;/code&gt; is from when upgrades were made to the OpenShift cluster (because an OpenShift update may have updated an OS setting in say &lt;code&gt;/etc/crio/crio.conf&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;As well as OCP version updates forcing updates, you can also manually edit &lt;code&gt;MachineConfigs&lt;/code&gt; to modify or add you own OS config files. For what it&#39;s worth, I did have to do this with early OpenShift 4.1/4.2 versions, where I modified NTP and SSHD config files for custom settings and to modify the ca-bundle.crt as a workaround as early versions had issues with private CAs. But since those early versions, I have not had need to make any changes - the MCO just manages the complete OS lifecycle of the node.&lt;/p&gt;
&lt;p&gt;When a new &lt;code&gt;MachineConfig&lt;/code&gt; is generated, the &lt;em&gt;machine-config-controller&lt;/em&gt; will work with the &lt;em&gt;machine-config-daemon&lt;/em&gt; running on each node to reboot it and have it apply the latest ignition configuration held within the &lt;code&gt;MachineConfig&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When a node boots Red Hat CoreOS, &lt;em&gt;ignition&lt;/em&gt; connects to the &lt;em&gt;machine-config-server&lt;/em&gt; endpoint &lt;code&gt;https://&amp;lt;cluster-api&amp;gt;:22623/config/master&lt;/code&gt; (or &lt;code&gt;/config/worker&lt;/code&gt; for worker nodes, or &lt;code&gt;/config/&amp;lt;machine-config-pool&amp;gt;&lt;/code&gt;) and applies the &lt;em&gt;Ignition&lt;/em&gt; config to the node.&lt;/p&gt;
&lt;h2 id=&#34;quick-health-check-of-mco&#34;&gt;Quick Health Check of MCO&lt;/h2&gt;
&lt;p&gt;Most of the time, the MCO works very well. However, as with any highly automated process, on occasions it does go wrong. The first thing to be aware of with MCO is that you have to keep a close eye on it, particularly when it&#39;s rolling out updates. At a high level, the quickest ways to do this is by looking at status of the the ClusterOperator&#39;s status:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc get co machine-config
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;NAME             VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;machine-config   4.8.10    True        False         False       14d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here we can see that the MCO is &lt;strong&gt;AVAILABLE=True&lt;/strong&gt;. This is good, and you nearly always want to see this. If it&#39;s &lt;strong&gt;False&lt;/strong&gt;, something is very wrong with the cluster, or more likely perhaps, the cluster is in the middle of an OCP upgrade and the MCO pod is restarting on a different node.&lt;/p&gt;
&lt;p&gt;If &lt;strong&gt;PROGRESSING=False&lt;/strong&gt; is shown, that means that the MCO has no updates to rollout, and that the nodes in your cluster are all up to date and running on the latest OS version and using all the OS config files from the &lt;code&gt;MachineConfig&lt;/code&gt;. If this is set to &lt;strong&gt;False&lt;/strong&gt;, then that means the &lt;em&gt;machine-config-controller&lt;/em&gt; is busy working through an update and asking the &lt;em&gt;machine-config-daemons&lt;/em&gt; on each node to reboot the node and apply a &lt;code&gt;MachineConfig&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For a healthy MCO, you also want to see &lt;strong&gt;DEGRADED=False&lt;/strong&gt;. If you see this as &lt;strong&gt;True&lt;/strong&gt;, then your MCO has a problem that usually requires manual intervention, and some of the key reasons that I&#39;ve seen for this condition are documented below.&lt;/p&gt;
&lt;p&gt;As well as looking at the overview status of the &lt;code&gt;ClusterOperator&lt;/code&gt; resource, another way you can see the health of the MCO at a glance is to look at its &lt;code&gt;MachineConfigPools&lt;/code&gt;. In a healthy cluster, these pools will show &lt;strong&gt;UPDATED=True&lt;/strong&gt; and &lt;strong&gt;DEGRADED=False&lt;/strong&gt;. If &lt;strong&gt;UPDATING=True&lt;/strong&gt; is set, the &lt;em&gt;machine-config-controller&lt;/em&gt; is in the middle of rolling out an update:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc get mcp
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;master   rendered-master-98968a4a639e3efad0436d19d4a6c93f   True      False      False      3              3                   3                     0                      216d
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;worker   rendered-worker-53656729c24a74935907c4fba2adc90a   True      False      False      3              3                   3                     0                      216d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;monitoring-an-mco-update&#34;&gt;Monitoring an MCO Update&lt;/h2&gt;
&lt;p&gt;If your MCO is updating (&lt;strong&gt;PROGRESSING=True&lt;/strong&gt;, &lt;strong&gt;UPDATING=True&lt;/strong&gt;), then you can monitor its progress by watching the logs of the &lt;em&gt;machine-config-controller&lt;/em&gt; pod.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc logs -l k8s-app=machine-config-controller -n openshift-machine-config-operator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the logs from this pod, you&#39;ll see nodes in the cluster sequentially being cordoned (made &lt;code&gt;schedulable=False&lt;/code&gt;) and then rebooted. Whilst rebooting and applying the new configuration, they will be seen as &lt;code&gt;NotReady&lt;/code&gt; status in &lt;code&gt;oc get nodes&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc get nodes
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;NAME       STATUS                        ROLES           AGE    VERSION
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;server01   Ready                         master,worker   217d   v1.21.1+9807387
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;server02   NotReady,SchedulingDisabled   master,worker   217d   v1.21.1+9807387
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;server03   Ready                         master,worker   217d   v1.21.1+9807387
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;server04   NotReady,SchedulingDisabled   worker          203d   v1.21.1+9807387
&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;server05   Ready                         worker          203d   v1.21.1+9807387
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If something happens when the node reboots (eg it fails to DHCP, it can&#39;t boot because of a disk or grub error, fails to start the kubelet), the MCO rollout process will halt. You&#39;ll notice this because the node stays &lt;code&gt;NotReady&lt;/code&gt; for longer than you&#39;d expect and the status will become &lt;em&gt;degraded&lt;/em&gt;. If this happens, you have to roll your sleeves up and work out why.&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting-degraded-nodes&#34;&gt;Troubleshooting Degraded Nodes&lt;/h2&gt;
&lt;p&gt;The first place to look when troubleshooting a &lt;em&gt;degraded&lt;/em&gt; node is its machine-config-daemon pod. In early OpenShift 4.x versions, there were also occasions when the reason for a degraded node was only apparent by looking in the Linux journald logs on the node itself. However in recent versions, I&#39;ve not had to resort to journald logs because everything about an error has been available in the &lt;em&gt;machine-config-daemon&lt;/em&gt; pod logs for the node. To find the correct &lt;em&gt;machine-config-daemon&lt;/em&gt; pod for the node, you simply run &lt;code&gt;oc get pods -o wide&lt;/code&gt;, or alternatively you could cut and paste the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ MCDPOD=$(oc get pods -l k8s-app=machine-config-daemon -o jsonpath=&amp;#39;{.items[?(@.spec.nodeName==&amp;#34;server01&amp;#34;)].metadata.name&amp;#39;)
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ oc logs $MCDPOD -c machine-config-daemon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, lets look at a few common reasons for a node being degraded.&lt;/p&gt;
&lt;h3 id=&#34;unexpected-on-disk-state---content-mismatch-for-file&#34;&gt;Unexpected on-disk state - content mismatch for file&lt;/h3&gt;
&lt;p&gt;If the files on a node&#39;s disk that are under control of the &lt;code&gt;MachineConfig&lt;/code&gt; don&#39;t match the contents of those same files as specified in that &lt;code&gt;MachineConfig&lt;/code&gt; (ie the &lt;code&gt;MachineConfig&lt;/code&gt; specified in the node&#39;s &lt;code&gt;machineconfiguration.openshift.io/currentConfig&lt;/code&gt; annotation), then the &lt;em&gt;machine-config-daemon&lt;/em&gt; will refuse to apply the update and this will put the &lt;code&gt;MachineConfigPool&lt;/code&gt; into a degraded state. You&#39;ll see a message event similar to this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;Type:                  RenderDegraded
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;Message:               Node server01 is reporting: &amp;#34;unexpected on-disk state validating against rendered-worker-82df13b281ba3ddb9ba2e8811ef7957c: content mismatch for file \&amp;#34;/etc/kubernetes/kubelet.conf\&amp;#34;&amp;#34;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;Reason:                1 nodes are reporting degraded status on sync
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;Status:                True
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;Type:                  NodeDegraded
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This happens when somebody has manually edited a file on a node. Although remember, there should be no reason or need for a sysadmin (SA) to edit files on RHCOS, but sometimes junior SAs or SAs not familiar with OpenShift might be tempted to edit OS config files using ssh/vim.&lt;/p&gt;
&lt;p&gt;You can get an indication of this by looking at the &lt;code&gt;machineconfiguration.openshift.io/ssh&lt;/code&gt; annotation on the node. If this is set to accessed, then a SA has ssh&#39;d onto the node and could have perhaps changed something manually. Ordinarily (except see troubleshooting below), SA shouldn&#39;t be ssh&#39;ing onto RHCOS nodes.&lt;/p&gt;
&lt;p&gt;The easiest way to quickly fix this is to restore the config file from the &lt;code&gt;MachineConfig&lt;/code&gt;. To do this, output the correct &lt;code&gt;MachineConfig&lt;/code&gt; in full YAML, and then search for the path entry for the file being reported as having a content mismatch. Alternatively, copy and run the following commands to get the file contents:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ CURRENT_CONFIG=$(oc get node -o jsonpath=&amp;#34;{.items[?(@.metadata.name==&amp;#39;server01&amp;#39;)].metadata.annotations[&amp;#39;machineconfiguration\.openshift\.io/currentConfig&amp;#39;]}&amp;#34;)
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ oc get mc $CURRENT_CONFIG -o jsonpath=&amp;#34;{.spec.config.storage.files[?(@.path==&amp;#39;/etc/kubernetes/kubelet.conf)].contents.source}&amp;#34; &amp;gt; newkubelet.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you simply base64 decode (or urldecode) the file, and scp it back onto the degraded node in the correct location. This will overwrite the manual changes made to the file and restore it to what the MCO wants it to be. The MCO should then pick up that the &lt;code&gt;MachineConfig&lt;/code&gt; now matches, and continue progressing with what it was doing.&lt;/p&gt;
&lt;h3 id=&#34;error-running-rpm-ostree&#34;&gt;Error running rpm-ostree&lt;/h3&gt;
&lt;p&gt;Red Hat CoreOS uses something called &lt;a href=&#34;&#34;&gt;rpm-ostree&lt;/a&gt; which is a &lt;em&gt;&amp;quot;hybrid image/package system&amp;quot;&lt;/em&gt;. It&#39;s basically an archive file that contain all the Linux software package (RPMS) that CoreOS requires and allows atomic upgrades (and downgrades) by being able to pivot between them. Sometimes this fails, and the node will become degraded. I&#39;ve seen this fail for a couple of reasons - once with an issue where the node was unable to download an image from quay.io, and another time when a third party vendor (who wasn&#39;t familiar with how rpm-ostrees and Red Hat CoreOS worked) set the immutable ACL attribute on some of its files on the node, causing the node to fail drastically on each OS upgrade. The vendor has since removed these custom ACLs from their product after our experience.&lt;/p&gt;
&lt;p&gt;The best way to fix this, if you can, is to correct the issue breaking the pivot. For example, by removing the immutable ACL on a system file. The Linux systemd journal on the node should help with identifying the error. If you still can&#39;t fix it, the last resort to fix this issue is to force a pivot.&lt;/p&gt;
&lt;h3 id=&#34;forcing-a-pivot&#34;&gt;Forcing a Pivot&lt;/h3&gt;
&lt;p&gt;If you can&#39;t get the node to update, you&#39;ll have to force it. To do this, you connect to the node and run the &lt;code&gt;machine-config-daemon pivot&lt;/code&gt; command with the correct OS image SHA (find this from the errors in the logs to be sure you get the correct one!)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc debug node/server01
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;Starting pod/server01-debug ...
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;To use host binaries, run `chroot /host`
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;Pod IP: 192.168.1.201
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;If you don&amp;#39;t see a command prompt, try pressing enter.
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;# sh-4.4
&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;# chroot /host
&lt;span class=&#34;ln&#34;&gt;8&lt;/span&gt;# /run/bin/machine-config-daemon pivot &amp;#34;quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:aaaabbbbccccdddd&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;forcing-a-node-to-use-a-specific-machineconfig&#34;&gt;Forcing a Node to Use a Specific MachineConfig&lt;/h3&gt;
&lt;p&gt;As a last resort - and experience has shown it really is better to address any MCO issues in other ways before doing this - you can force a node to reboot and use a different &lt;code&gt;MachineConfig&lt;/code&gt;. This is done by manually editing the machineconfiguration annotations on the Node object itself.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;oc patch node server01 --type merge --patch &amp;#39;{&amp;#34;metadata&amp;#34;: {&amp;#34;annotations&amp;#34;: {&amp;#34;machineconfiguration.openshift.io/currentConfig&amp;#34;: &amp;#34;render-worker-52656729c24a7493&amp;#34;}}}&amp;#39;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;oc patch node server01 --type merge --patch &amp;#39;{&amp;#34;metadata&amp;#34;: {&amp;#34;annotations&amp;#34;: {&amp;#34;machineconfiguration.openshift.io/desiredConfig&amp;#34;: &amp;#34;render-worker-52656729c24a7493&amp;#34;}}}&amp;#39;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;oc patch node server01 --type merge --patch &amp;#39;{&amp;#34;metadata&amp;#34;: {&amp;#34;annotations&amp;#34;: {&amp;#34;machineconfiguration.openshift.io/reason&amp;#34;: &amp;#34;&amp;#34;}}}&amp;#39;
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;oc patch node server01 --type merge --patch &amp;#39;{&amp;#34;metadata&amp;#34;: {&amp;#34;annotations&amp;#34;: {&amp;#34;machineconfiguration.openshift.io/state&amp;#34;: &amp;#34;Done&amp;#34;}}}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then you have to connect to the node and touch a machine-config-daemon-force file. This will trigger the &lt;em&gt;machine-config-daemon&lt;/em&gt; to do a reboot and to force the update to the &lt;code&gt;MachineConfig&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc debug node/server01
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;[root@server01 core] chroot /host
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;[root@server01 core] touch /run/machine-config-daemon-force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;On occasion, you might not be able to connect to the node using an oc debug pod. I&#39;ve seen this happen when a systemd unit running podman early in the boot process has had an image pull failure, and the debug pod is also not able to run when kubelet is not running properly, mean SSH is the only option.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ ssh core@server01
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;[root@server01 core] touch /run/machine-config-daemon-force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pausing-mco-updates&#34;&gt;Pausing MCO Updates&lt;/h2&gt;
&lt;p&gt;As described above, the MCO will automatically carry on the work of rebooting and managing your nodes if you let it. This is great, but does mean that nodes in the cluster can sometimes be rebooted without warning. Of course, true cloud-native Kubernetes ready applications should be running multiple replicas across nodes anyway, and this shouldn&#39;t normally be a problem.&lt;/p&gt;
&lt;p&gt;However, should you wish to pause the MCO so that it doesn&#39;t automatically reboot nodes when you don&#39;t want it to, you can &amp;quot;pause&amp;quot; the worker pools by setting the &lt;code&gt;.spec.paused&lt;/code&gt; field in the &lt;code&gt;MachineConfigPool&lt;/code&gt;. For example, by running the following patch commands:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ oc patch mcp master --type merge --patch &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;paused&amp;#34;: true}}&amp;#39;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ oc patch mcp worker --type merge --patch &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;paused&amp;#34;: true}}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It should be noted that you shouldn&#39;t leave the &lt;code&gt;MachineConfigPool&#39;s&lt;/code&gt; paused for long periods of times. This can cause issues for MCO if configuration updates combine with cluster cert rotation events. However, pausing them whilst the actual OCP control plane upgrade happens and then unpausing when you&#39;re ready for all the nodes to be rebooted can be very useful.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The Machine Config Operator is a clever piece of software that significantly reduces workload of running and managing nodes in an OpenShift cluster. However, it&#39;s important for cluster administrators to understand how it works, and to be able to successfully recover it when any issues arise and it becomes degraded. Hopefully, this blog post will help you understand it a little better and to resolve any issues you encounter.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
